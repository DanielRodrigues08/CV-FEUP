# -*- coding: utf-8 -*-
"""pretrained_cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11BwNJom79Urc3FpYdoGd_WHVu9n4vrew
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import mean_absolute_error
import torch.nn.functional as F
import os
import json
from torchsummary import summary
import random
import numpy as np
import matplotlib.pyplot as plt
random.seed(42)

base_folder = "../"

models_folder = base_folder + "/models/"
plot_data = base_folder + "/plot_data/"
image_paths_file = base_folder + "/image_paths.txt"

if not os.path.exists(models_folder):
    os.makedirs(models_folder)

if not os.path.exists(plot_data):
    os.makedirs(plot_data)

"""# **Helper Functions:**"""

def draw_bar_plot(labels):
    unique_labels = list(set(labels))
    counts = [labels.count(label) for label in unique_labels]

    plt.bar(range(len(unique_labels)), counts, color='skyblue')
    plt.xlabel('Label')
    plt.ylabel('Frequency')
    plt.title('Bar Plot of Labels')
    plt.xticks(range(len(unique_labels)), unique_labels)  # Set x-axis labels
    plt.show()

def list_files(folder_path):
    return [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)]

def get_files():
  image_paths = []
  if os.path.exists(image_paths_file):
    with open(image_paths_file, "r") as file:
      lines = file.readlines()
      # Strip newline characters and append to list
      image_paths = [line.strip() for line in lines]
  else:
    image_paths.extend(list_files(f"{base_folder}/drive_dataset/"))
    for chunk_id in range(1, 7+1):
      image_paths.extend(list_files(f"{base_folder}/generated_dataset/chunk_{chunk_id}"))
    with open(image_paths_file, "w") as file:
      file.writelines(path + "\n" for path in image_paths)
  return image_paths

def plotTrainingHistory(train_history, val_history):
    plt.subplot(2, 1, 1)
    plt.title('Cross Entropy Loss')
    plt.plot(train_history['loss'], label='train')
    plt.plot(val_history['loss'], label='val')
    plt.legend(loc='best')

    plt.subplot(2, 1, 2)
    plt.title('Classification Accuracy')
    plt.plot(train_history['accuracy'], label='train')
    plt.plot(val_history['accuracy'], label='val')

    plt.tight_layout()
    plt.legend(loc='best')
    plt.show()

def save_dict_to_file(data, filename):
  with open(filename, 'w') as file:
    json.dump(data, file)

def get_saved_dict(filename):
  with open(filename, 'r') as file:
      data = json.load(file)
  return data

"""# **Tranformers**"""

resnet_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

transform = resnet_transform

"""# **Dataset:**"""

batch_size = 32
num_workers = 2
train_size = 0.8
validation_size = 0.1
test_size = 0.1

class LegoDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert("RGB")
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

image_paths = get_files()

image_paths = list(filter(lambda x: "(" not in x, image_paths))
random.shuffle(image_paths)

labels = list(map(lambda x: int(x[x.rfind('_')+1:x.rfind('.')]), image_paths))

draw_bar_plot(labels)

image_paths = np.asarray(image_paths)
labels = np.asarray(labels)

# Randomly split data into train (0), validation (1) and test (2) sets
split = np.random.choice([0, 1, 2], len(image_paths), p=[train_size, validation_size, test_size])

train_indexes = np.where(split == 0)[0]
valid_indexes = np.where(split == 1)[0]
test_indexes = np.where(split == 2)[0]

train_dataset = LegoDataset(image_paths[train_indexes], labels[train_indexes], transform=transform)
valid_dataset = LegoDataset(image_paths[valid_indexes], labels[valid_indexes], transform=transform)
test_dataset = LegoDataset(image_paths[test_indexes], labels[test_indexes], transform=transform)

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)
valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)

"""# **Model:**"""

import torchvision.models as models

"""## ResNet50"""

## Load ResNet model from torchvision (with pretrained=True)
#resnet = models.resnet50(pretrained=True)
#
## Disable Gradients
#for param in resnet.parameters():
#    param.requires_grad = False
#
## Change the number of neurons in the last layer to the number of classes of the CIFAR10 dataset
#num_ftrs = resnet.fc.in_features
#resnet.fc = nn.Linear(num_ftrs, 1)

"""## VGG16"""

#from torchvision.models import VGG16_Weights
#
#vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)
#
#for param in vgg16.parameters():
#    param.requires_grad = False
#
#num_ftrs = vgg16.classifier[6].in_features
#vgg16.classifier[6] = nn.Linear(num_ftrs, 1)
#
#for param in vgg16.classifier[6].parameters():
#    param.requires_grad = True

"""## DenseNet"""

from torchvision.models import DenseNet201_Weights

densenet = models.densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)

for param in densenet.parameters():
    param.requires_grad = False

num_ftrs = densenet.classifier.in_features
densenet.classifier = nn.Linear(num_ftrs, 1)

for param in densenet.classifier.parameters():
    param.requires_grad = True

model = densenet
best_model_file = models_folder + "densenet_0001_un_best_model.pth"
latest_model_file = models_folder + "densenet_0001_un_latest_model.pth"

train_history_file = plot_data + "densenet_0001_un_train_history.json"
val_history_file = plot_data + "densenet_0001_un_val_history.json"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

learning_rate = 0.0001
loss = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

"""# **Train the model:**"""

# Define the training loop
def epoch_iter(dataloader, model, loss_fn, optimizer=None, is_train=True):
    if is_train:
        assert optimizer is not None, "When training, please provide an optimizer."

    # Get number of batches
    num_batches = len(dataloader)

    # Set model to train mode or evaluation mode
    if is_train:
        model.train()
    else:
        model.eval()

    # Define variables to save predictions and labels during the epoch
    total_loss = 0.0
    preds = []
    labels = []

    # Enable/disable gradients based on whether the model is in train or evaluation mode
    with torch.set_grad_enabled(is_train):

        # Analyse all batches
        for batch, (X, y) in enumerate(tqdm(dataloader)):

            # Put data in same device as model (GPU or CPU)
            X, y = X.to(device), y.to(device)

            # Forward pass to obtain prediction of the model
            pred = model(X)

            # Compute loss between prediction and ground-truth
            loss = loss_fn(pred, y.float().unsqueeze(1))  # Convert labels to float for regression task

            # Backward pass
            if is_train:
                # Reset gradients in optimizer
                optimizer.zero_grad()
                # Calculate gradients by backpropagating loss
                loss.backward()
                # Update model weights based on the calculated gradients
                optimizer.step()

            # Save training metrics
            total_loss += loss.item()  # IMPORTANT: call .item() to obtain the value of the loss WITHOUT the computational graph attached

            # Add predictions
            preds.extend(pred.detach().cpu().numpy().astype(int))
            labels.extend(y.cpu().numpy())

    return total_loss / num_batches, mean_absolute_error(labels, preds), labels, preds

def train(model, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer, num_epochs_to_unfreeze = -1, train_values = None, val_values = None, resume = False):
    train_history = None
    val_history = None
    best_val_loss = None
    init_epochs = 0

    if resume:
        train_history = train_values
        val_history = val_values
        best_val_loss = min(val_history['loss'])
        init_epochs = len(val_history['loss']) - 1
    else:
        train_history = {'loss': [], 'accuracy': []}
        val_history = {'loss': [], 'accuracy': []}
        best_val_loss = float('inf')


    print("Start training...")

    for t in range(init_epochs, num_epochs):
        print(f"\nEpoch {t+1}")

        if t + 1 == num_epochs_to_unfreeze:
            for param in model.parameters():
              param.requires_grad = True
            print("Switched to MSE")
            loss_fn = nn.MSELoss()

        # Train model for one iteration on training data
        train_loss, train_acc, _a, _b = epoch_iter(train_dataloader, model, loss, optimizer)
        print(f"Train loss: {train_loss:.3f} \t Train acc: {train_acc:.3f}")

        # Evaluate model on validation data
        val_loss, val_acc, _a, _b = epoch_iter(valid_dataloader, model, loss, None, is_train=False)
        print(f"Val loss: {val_loss:.3f} \t Val acc: {val_acc:.3f}")

        if val_loss < 1:
          print("Switched to MAE: " + str(val_loss))
          loss_fn = nn.L1Loss()

        # Save model when validation loss improves
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}
            torch.save(save_dict, best_model_file)

        # Save latest model
        save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}
        torch.save(save_dict, latest_model_file)

        # Save training history for plotting purposes
        train_history["loss"].append(train_loss)
        train_history["accuracy"].append(train_acc)

        val_history["loss"].append(val_loss)
        val_history["accuracy"].append(val_acc)

        save_dict_to_file(train_history, train_history_file)
        save_dict_to_file(val_history, val_history_file)

    print("Finished")
    return train_history, val_history

num_epochs = 50
num_epochs_to_unfreeze = 20

resume = False
train_values = None
val_values = None

#train_values = get_saved_dict('/content/drive/Shareddrives/VC/plot_data/pretrained/densenet_0001_un_train_history.json')
#val_values = get_saved_dict('/content/drive/Shareddrives/VC/plot_data/pretrained/densenet_0001_un_val_history.json')
#
#checkpoint = torch.load('/content/drive/Shareddrives/VC/models/pretrained/densenet_0001_un_latest_model.pth')
#model.load_state_dict(checkpoint['model'])

train_history, val_history = train(model, num_epochs, train_dataloader, valid_dataloader, loss, optimizer, num_epochs_to_unfreeze, train_values, val_values, resume)

plotTrainingHistory(train_history, val_history)


"""# Test the model"""

from sklearn.metrics import accuracy_score

checkpoint = torch.load(best_model_file)
model.load_state_dict(checkpoint['model'])

test_loss, test_acc, labels, preds = epoch_iter(test_dataloader, model, loss, is_train=False)
print(f'\nTest Loss: {test_loss:.3f} \nTest Accuracy: {test_acc:.3f}')

accuracy = accuracy_score(labels, preds)
print(f"Accuracy: {accuracy:.2f}")

