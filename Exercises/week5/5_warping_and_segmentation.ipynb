{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Warping and Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "imagesDir = '../Images_03a' # Change this, according to your images' directory path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'giraffe.jpg')) # Change this, according to your image's path\n",
    "\n",
    "# Resize image to facilitate visualization\n",
    "img = cv2.resize(img, (0, 0), fx = 0.4, fy = 0.4)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select original coordinates of three points of the original image\n",
    "ori_coord = np.array([[0, 0], [img.shape[1] - 1, 0], [0, img.shape[0] - 1]]).astype(np.float32)\n",
    "\n",
    "# Select target coordinates (where the points will move to)\n",
    "tar_coord = np.array([[0, img.shape[1]*0.33], [img.shape[1]*0.85, img.shape[0]*0.25], [img.shape[1]*0.15, img.shape[0]*0.7]]).astype(np.float32)\n",
    "\n",
    "# Get affine transformation matrix\n",
    "warp_mat = cv2.getAffineTransform(ori_coord, tar_coord)\n",
    "\n",
    "# Apply transformation to the image\n",
    "warp_dst = cv2.warpAffine(img, warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# Show Image\n",
    "cv2.imshow('Warped Image', warp_dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.1: Rotate an image by [defining rotation matrix](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gafbbc470ce83812914a70abfb604f4326) and then applying transformation to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = img.shape[:2]\n",
    "\n",
    "rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), 45, 1)\n",
    "\n",
    "rotated_img = cv2.warpAffine(img, rotation_matrix, (img.shape[1], img.shape[0]))\n",
    "\n",
    "cv2.imshow('Rotated Image', rotated_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.2: Rotate an image through an Affine Transformation instead of a rotation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "# Select original coordinates of three points of the original image\n",
    "ori_coord = np.array([[0, 0], [h, 0], [h, w]]).astype(np.float32)\n",
    "\n",
    "angle = -45 * math.pi / 180\n",
    "\n",
    "# Select target coordinates (where the points will move to)\n",
    "tar_coord = np.array([[0 , w], # x, 0\n",
    "                      [0, 0], # x, y\n",
    "                      [h, 0]]).astype(np.float32) # 0, 0\n",
    "\n",
    "# Get affine transformation matrix\n",
    "warp_mat = cv2.getAffineTransform(ori_coord, tar_coord)\n",
    "\n",
    "# Apply transformation to the image\n",
    "warp_dst = cv2.warpAffine(img, warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# Show Image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.imshow('Warped Image', warp_dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.3: Apply a translation of 100 pixels to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homography\n",
    "\n",
    "Example of homography using feature matching from last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "img1 = cv2.imread(os.path.join(imagesDir, 'match_box01a_1.png'), cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(os.path.join(imagesDir, 'match_box01a_2.png'), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "cv2.imshow('Query', img1)\n",
    "cv2.imshow('Train', img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "# Apply FLAN Matcher\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "# Stop if no matches are found\n",
    "if len(good) < 10:\n",
    "    print( \"Error: Insufficient number of matches\")\n",
    "    exit(-1)\n",
    "\n",
    "# Obtain points corresponding to the matches in the query and train images\n",
    "query_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "train_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "# Obtain homography that represents the transformation from the points of the train image into the position of the query image \n",
    "M, mask = cv2.findHomography(train_pts, query_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Apply transformation to image\n",
    "warped_img = cv2.warpPerspective(img2, M, (img1.shape[1], img1.shape[0]),flags=cv2.INTER_LINEAR)\n",
    "\n",
    "cv2.imshow('Query', img1)\n",
    "cv2.imshow('Original Image', img2)\n",
    "cv2.imshow('Warped Image', warped_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.4: Draw lines around the object by:\n",
    "* Obtaining homography that transforms points from the query image to the train image\n",
    "* Applying [the perspectiveTransform function](https://docs.opencv.org/3.4/d2/de8/group__core__array.html#gad327659ac03e5fd6894b90025e6900a7) to obtain the coordinates of the object of the query image on the train image\n",
    "* Drawing lines on the train image that connect the coordinates of the object using [polylines](https://docs.opencv.org/3.4/d6/d6e/group__imgproc__draw.html#gaa3c25f9fb764b6bef791bf034f6e26f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "pts = np.array([[0, 0], [0, img1.shape[0] - 1], [img1.shape[1] - 1, img1.shape[0] - 1], [img1.shape[1] - 1, 0]]).astype(np.float32)\n",
    "\n",
    "dst = cv2.perspectiveTransform(pts.reshape(-1, 1, 2), M)\n",
    "\n",
    "img2_bgr = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "img2_lines = cv2.polylines(img2_bgr, [np.int32(dst)], True, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('Image', img2_lines)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Segmentation\n",
    "\n",
    "### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'sudoku.png'))\n",
    "\n",
    "# Convert to grayscale\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otsu Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply global binary threshold\n",
    "ret, th_global = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Apply binary threshold with Otsu's method\n",
    "ret, th_otsu = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('Global Threshold', th_global)\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.1: Verify the effects of blurring the image using a Gaussian filter, before applying the Otsu thresholding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply gaussian filter\n",
    "img_blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "# Apply binary threshold with Otsu's method\n",
    "ret, th_otsu_blur = cv2.threshold(img_blur, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "# Apply binary threshold with Otsu's method\n",
    "ret, th_otsu = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('Original Image', img)\n",
    "cv2.imshow('Gaussian Blur', th_otsu_blur)\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply adaptive thresholding\n",
    "th_adaptive = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('Adaptive Mean', th_adaptive)\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.imshow('Global Threshold', th_global)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.2: Verify the effects of blurring with filters of increasing sizes before applying the adaptive threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation with [K-Means](https://docs.opencv.org/master/d5/d38/group__core__cluster.html#ga9a34dc06c6ec9460e90860f15bcd2f88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img2 = cv2.imread(os.path.join(imagesDir, 'home.jpg'))\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: (384, 512, 3)\n",
      "Current shape: (196608, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the image and turn its values to float\n",
    "print(f\"Previous shape: {img2.shape}\")\n",
    "\n",
    "reshaped_image = img2.reshape((-1,3))\n",
    "reshaped_image = np.float32(reshaped_image)\n",
    "\n",
    "print(f\"Current shape: {reshaped_image.shape}\")\n",
    "\n",
    "# Define criteria and number of clusters (k)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "k = 4\n",
    "\n",
    "# Apply K-means\n",
    "ret, label, center = cv2.kmeans(reshaped_image, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to uint8, and make resulting image\n",
    "center = np.uint8(center)\n",
    "result = center[label.flatten()]\n",
    "result = result.reshape((img2.shape))\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.imshow('K-Means Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.3: Experiment with different number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "\n",
    "# Apply K-means\n",
    "ret, label, center = cv2.kmeans(reshaped_image, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "center = np.uint8(center)\n",
    "result = center[label.flatten()]\n",
    "result = result.reshape((img2.shape))\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.imshow('K-Means Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation with [GrabCut](https://docs.opencv.org/4.x/d3/d47/group__imgproc__segmentation.html#ga909c1dda50efcbeaa3ce126be862b37f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'giraffe.jpg')) # Change this, according to your image's path\n",
    "\n",
    "# Resize image to facilitate visualization\n",
    "img = cv2.resize(img, (0, 0), fx = 0.6, fy = 0.6)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define image mask for the GrabCut output with same dimensions as the image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[43mimg\u001b[49m\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the bounding box coordinates with the object of interest: (x, y, width, heigh)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m bb \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m500\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "# Define image mask for the GrabCut output with same dimensions as the image\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "# Define the bounding box coordinates with the object of interest: (x, y, width, heigh)\n",
    "bb = (0, 0, 400, 500)\n",
    "\n",
    "# Allocate memory for the two arrays that this algorithm internally uses for the segmentation of the foreground and background\n",
    "bgModel = np.zeros((1, 65), np.float64)\n",
    "fgModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Apply GrabCut\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(img, mask, bb, bgModel, fgModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# All definite background and probable background pixels are set to 0, and all definite foreground and probable foreground pixels are set to 1\n",
    "output_mask = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "\n",
    "# Scale the mask from the range [0, 1] to [0, 255]\n",
    "output_mask = (output_mask * 255).astype(\"uint8\")\n",
    "\n",
    "# Apply a bitwise AND to the image using the generated mask by GrabCut to obtain the final result\n",
    "grabcut_result = cv2.bitwise_and(img, img, mask=output_mask)\n",
    "\n",
    "# Show result\n",
    "cv2.imshow('Output Mask', output_mask)\n",
    "cv2.imshow('GrabCut Result', grabcut_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.4: Select a region of interest in the image (using cv2.selectROI function) for the GrabCut algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r = cv2.selectROI(img)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "x = int(r[0])\n",
    "y = int(r[1])\n",
    "width = int(r[2])\n",
    "height = int(r[3])\n",
    "\n",
    "# Define image mask for the GrabCut output with same dimensions as the image\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "# Define the bounding box coordinates with the object of interest: (x, y, width, heigh)\n",
    "bb = (x, y, width, height)\n",
    "\n",
    "# Allocate memory for the two arrays that this algorithm internally uses for the segmentation of the foreground and background\n",
    "bgModel = np.zeros((1, 65), np.float64)\n",
    "fgModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Apply GrabCut\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(img, mask, bb, bgModel, fgModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# All definite background and probable background pixels are set to 0, and all definite foreground and probable foreground pixels are set to 1\n",
    "output_mask = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "\n",
    "# Scale the mask from the range [0, 1] to [0, 255]\n",
    "output_mask = (output_mask * 255).astype(\"uint8\")\n",
    "\n",
    "# Apply a bitwise AND to the image using the generated mask by GrabCut to obtain the final result\n",
    "grabcut_result = cv2.bitwise_and(img, img, mask=output_mask)\n",
    "\n",
    "# Show result\n",
    "cv2.imshow('Output Mask', output_mask)\n",
    "cv2.imshow('GrabCut Result', grabcut_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
