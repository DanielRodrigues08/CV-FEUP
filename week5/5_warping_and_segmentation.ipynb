{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Warping and Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:11:26.558156320Z",
     "start_time": "2024-03-15T11:11:26.250092046Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "imagesDir = '../Images_03a' # Change this, according to your images' directory path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'giraffe.jpg')) # Change this, according to your image's path\n",
    "\n",
    "# Resize image to facilitate visualization\n",
    "img = cv2.resize(img, (0, 0), fx = 0.4, fy = 0.4)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select original coordinates of three points of the original image\n",
    "ori_coord = np.array([[0, 0], [img.shape[1] - 1, 0], [0, img.shape[0] - 1]]).astype(np.float32)\n",
    "\n",
    "# Select target coordinates (where the points will move to)\n",
    "tar_coord = np.array([[0, img.shape[1]*0.33], [img.shape[1]*0.85, img.shape[0]*0.25], [img.shape[1]*0.15, img.shape[0]*0.7]]).astype(np.float32)\n",
    "\n",
    "# Get affine transformation matrix\n",
    "warp_mat = cv2.getAffineTransform(ori_coord, tar_coord)\n",
    "\n",
    "# Apply transformation to the image\n",
    "warp_dst = cv2.warpAffine(img, warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# Show Image\n",
    "cv2.imshow('Warped Image', warp_dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.1: Rotate an image by [defining rotation matrix](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gafbbc470ce83812914a70abfb604f4326) and then applying transformation to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 720, 3)\n"
     ]
    }
   ],
   "source": [
    "# Specify the rotation angle\n",
    "angle = 45  # Rotate 45 degrees clockwise\n",
    "\n",
    "# Get image dimensions\n",
    "height, width = img.shape[:2]\n",
    "print(img.shape)\n",
    "\n",
    "# Define the rotation matrix\n",
    "rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)\n",
    "\n",
    "# Apply the rotation transformation\n",
    "rotated_image = cv2.warpAffine(img, rotation_matrix, (width, height))\n",
    "\n",
    "# Display the original and rotated images\n",
    "cv2.imshow('Original Image', img)\n",
    "cv2.imshow('Rotated Image', rotated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.2: Rotate an image through an Affine Transformation instead of a rotation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select original coordinates of three points of the original image\n",
    "ori_coord = np.array([[0, 0], [height, 0], [0, width]]).astype(np.float32)\n",
    "\n",
    "# Select target coordinates (where the points will move to)\n",
    "tar_coord = np.array([[height, 0], [height, width], [0, 0]]).astype(np.float32)\n",
    "\n",
    "# Get affine transformation matrix\n",
    "warp_mat = cv2.getAffineTransform(ori_coord, tar_coord)\n",
    "\n",
    "# Apply transformation to the image\n",
    "warp_dst = cv2.warpAffine(img, warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# Show Image\n",
    "cv2.imshow('Warped Image', warp_dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.3: Apply a translation of 100 pixels to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_matrix = np.float32([[1, 0, 100], [0, 1, 0]])  # Translate 100 pixels to the right\n",
    "\n",
    "# Apply the translation transformation\n",
    "translated_image = cv2.warpAffine(img, translation_matrix, (width, height))\n",
    "\n",
    "# Display the original and translated images\n",
    "cv2.imshow('Original Image', img)\n",
    "cv2.imshow('Translated Image', translated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homography\n",
    "\n",
    "Example of homography using feature matching from last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "img1 = cv2.imread(os.path.join(imagesDir, 'match_box01a_1.png'), cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(os.path.join(imagesDir, 'match_box01a_2.png'), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "cv2.imshow('Query', img1)\n",
    "cv2.imshow('Train', img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "# Apply FLAN Matcher\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "# Stop if no matches are found\n",
    "if len(good) < 10:\n",
    "    print( \"Error: Insufficient number of matches\")\n",
    "    exit(-1)\n",
    "\n",
    "# Obtain points corresponding to the matches in the query and train images\n",
    "query_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "train_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "# Obtain homography that represents the transformation from the points of the train image into the position of the query image \n",
    "M, mask = cv2.findHomography(train_pts, query_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Apply transformation to image\n",
    "warped_img = cv2.warpPerspective(img2, M, (img1.shape[1], img1.shape[0]),flags=cv2.INTER_LINEAR)\n",
    "\n",
    "cv2.imshow('Query', img1)\n",
    "cv2.imshow('Original Image', img2)\n",
    "cv2.imshow('Warped Image', warped_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.4: Draw lines around the object by:\n",
    "* Obtaining homography that transforms points from the query image to the train image\n",
    "* Applying [the perspectiveTransform function](https://docs.opencv.org/3.4/d2/de8/group__core__array.html#gad327659ac03e5fd6894b90025e6900a7) to obtain the coordinates of the object of the query image on the train image\n",
    "* Drawing lines on the train image that connect the coordinates of the object using [polylines](https://docs.opencv.org/3.4/d6/d6e/group__imgproc__draw.html#gaa3c25f9fb764b6bef791bf034f6e26f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "# Apply FLAN Matcher\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "# Stop if no matches are found\n",
    "if len(good) < 10:\n",
    "    print( \"Error: Insufficient number of matches\")\n",
    "    exit(-1)\n",
    "\n",
    "# Obtain points corresponding to the matches in the query and train images\n",
    "query_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "train_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "# Obtain homography that represents the transformation from the points of the train image into the position of the query image \n",
    "M, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "pts = np.float32([[0,0], [0,img1.shape[0]-1], [img1.shape[1]-1, img1.shape[0]-1], [img1.shape[1]-1,0]]).reshape(-1, 1, 2)\n",
    "\n",
    "dst = cv2.perspectiveTransform(pts, M)\n",
    "\n",
    "img2_bgr = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "img2_lines = cv2.polylines(img2_bgr, [np.int32(dst)], True, (255, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('Highlight', img2_lines)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Segmentation\n",
    "\n",
    "### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'sudoku.png'))\n",
    "\n",
    "# Convert to grayscale\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otsu Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply global binary threshold\n",
    "ret, th_global = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Apply binary threshold with Otsu's method\n",
    "ret, th_otsu = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('Global Threshold', th_global)\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.1: Verify the effects of blurring the image using a Gaussian filter, before applying the Otsu thresholding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "blured = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "ret, th_otsu = cv2.threshold(blured, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply adaptive thresholding\n",
    "th_adaptive = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('Adaptive Mean', th_adaptive)\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.imshow('Global Threshold', th_global)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.2: Verify the effects of blurring with filters of increasing sizes before applying the adaptive threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation with [K-Means](https://docs.opencv.org/master/d5/d38/group__core__cluster.html#ga9a34dc06c6ec9460e90860f15bcd2f88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img2 = cv2.imread(os.path.join(imagesDir, 'home.jpg'))\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: (384, 512, 3)\n",
      "Current shape: (196608, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the image and turn its values to float\n",
    "print(f\"Previous shape: {img2.shape}\")\n",
    "\n",
    "reshaped_image = img2.reshape((-1,3))\n",
    "reshaped_image = np.float32(reshaped_image)\n",
    "\n",
    "print(f\"Current shape: {reshaped_image.shape}\")\n",
    "\n",
    "# Define criteria and number of clusters (k)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "k = 4\n",
    "\n",
    "# Apply K-means\n",
    "ret, label, center = cv2.kmeans(reshaped_image, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to uint8, and make resulting image\n",
    "center = np.uint8(center)\n",
    "result = center[label.flatten()]\n",
    "result = result.reshape((img2.shape))\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.imshow('K-Means Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.3: Experiment with different number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: (384, 512, 3)\n",
      "Current shape: (196608, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read image\n",
    "img2 = cv2.imread(os.path.join(imagesDir, 'home.jpg'))\n",
    "\n",
    "# Reshape the image and turn its values to float\n",
    "print(f\"Previous shape: {img2.shape}\")\n",
    "\n",
    "reshaped_image = img2.reshape((-1,3))\n",
    "reshaped_image = np.float32(reshaped_image)\n",
    "\n",
    "print(f\"Current shape: {reshaped_image.shape}\")\n",
    "\n",
    "# Define criteria and number of clusters (k)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "k = 16\n",
    "\n",
    "# Apply K-means\n",
    "ret, label, center = cv2.kmeans(reshaped_image, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "# Convert back to uint8, and make resulting image\n",
    "center = np.uint8(center)\n",
    "result = center[label.flatten()]\n",
    "result = result.reshape((img2.shape))\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.imshow('K-Means Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation with [GrabCut](https://docs.opencv.org/4.x/d3/d47/group__imgproc__segmentation.html#ga909c1dda50efcbeaa3ce126be862b37f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'giraffe.jpg')) # Change this, according to your image's path\n",
    "\n",
    "# Resize image to facilitate visualization\n",
    "img = cv2.resize(img, (0, 0), fx = 0.6, fy = 0.6)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image mask for the GrabCut output with same dimensions as the image\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "# Define the bounding box coordinates with the object of interest: (x, y, width, heigh)\n",
    "bb = (0, 0, 400, 500)\n",
    "\n",
    "# Allocate memory for the two arrays that this algorithm internally uses for the segmentation of the foreground and background\n",
    "bgModel = np.zeros((1, 65), np.float64)\n",
    "fgModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Apply GrabCut\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(img, mask, bb, bgModel, fgModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# All definite background and probable background pixels are set to 0, and all definite foreground and probable foreground pixels are set to 1\n",
    "output_mask = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "\n",
    "# Scale the mask from the range [0, 1] to [0, 255]\n",
    "output_mask = (output_mask * 255).astype(\"uint8\")\n",
    "\n",
    "# Apply a bitwise AND to the image using the generated mask by GrabCut to obtain the final result\n",
    "grabcut_result = cv2.bitwise_and(img, img, mask=output_mask)\n",
    "\n",
    "# Show result\n",
    "cv2.imshow('Output Mask', output_mask)\n",
    "cv2.imshow('GrabCut Result', grabcut_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.4: Select a region of interest in the image (using cv2.selectROI function) for the GrabCut algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
